{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import sentencepiece as spm\n",
    "import re\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.0.57', '1.1.0')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fastai, torch\n",
    "fastai.__version__ , torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gaurav/PycharmProjects/nlp-for-panjabi/language-model\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/home/gaurav/PycharmProjects/nlp-for-panjabi/language-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from inltk.tokenizer import PanjabiTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inltk.tokenizer.PanjabiTokenizer"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PanjabiTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PanjabiTokenizer(BaseTokenizer):\n",
    "    def __init__(self, lang:str):\n",
    "        self.lang = lang\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.Load(str(path/\"../tokenizer/panjabi_lm.model\"))\n",
    "        \n",
    "    def tokenizer(self, t:str) -> List[str]:\n",
    "        return self.sp.EncodeAsPieces(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/gaurav/PycharmProjects/nlp-for-panjabi/language-model')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(str(path/\"../tokenizer/panjabi_lm.model\"))\n",
    "itos = [sp.IdToPiece(int(i)) for i in range(30000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<s>',\n",
       " '</s>',\n",
       " '।',\n",
       " ',',\n",
       " '▁ਹੈ',\n",
       " '▁ਦੇ',\n",
       " '▁',\n",
       " '▁ਵਿੱਚ',\n",
       " '▁ਦੀ',\n",
       " '▁ਅਤੇ',\n",
       " '▁ਦਾ',\n",
       " '▁ਨੂੰ',\n",
       " '▁ਇੱਕ',\n",
       " '▁ਹਨ',\n",
       " '-',\n",
       " '▁ਤੋਂ',\n",
       " '.',\n",
       " '▁ਸੀ',\n",
       " '▁ਤੇ']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PanjabiVocab():\n",
    "#     def __init__(self, itos:Collection[str]):\n",
    "#         self.sp = spm.SentencePieceProcessor()\n",
    "#         self.sp.Load(str(path/\"panjabi_lm.model\"))\n",
    "#         self.itos = [self.sp.IdToPiece(int(i)) for i in range(30000)]\n",
    "#         self.stoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.itos)})\n",
    "\n",
    "#     def numericalize(self, t:Collection[str]) -> List[int]:\n",
    "#         \"Convert a list of tokens `t` to their ids.\"\n",
    "#         print(t)\n",
    "#         return [self.sp.PieceToId(w) for w in t]\n",
    "\n",
    "# #     Sentence piece has its own separator in tokens i.e _\n",
    "#     def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n",
    "#         \"Convert a list of `nums` to their tokens.\"\n",
    "#         return sep.join([self.sp.IdToPiece(int(i)) for i in nums])\n",
    "\n",
    "# #     def __getstate__(self):\n",
    "# #         return {'itos':self.itos}\n",
    "\n",
    "# #     def __setstate__(self, state:dict):\n",
    "# #         self.itos = state['itos']\n",
    "# #         self.stoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.itos)})\n",
    "\n",
    "# #     @classmethod\n",
    "# #     def create(cls, tokens:Tokens, max_vocab:int, min_freq:int) -> 'Vocab':\n",
    "# #         \"Create a vocabulary from a set of `tokens`.\"\n",
    "# #         freq = Counter(p for o in tokens for p in o)\n",
    "# #         itos = [o for o,c in freq.most_common(max_vocab) if c > min_freq]\n",
    "# #         for o in reversed(defaults.text_spec_tok):\n",
    "# #             if o in itos: itos.remove(o)\n",
    "# #             itos.insert(0, o)\n",
    "# #         return cls(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30,000 is the vocab size that we chose in sentencepiece\n",
    "panjabi_vocab = Vocab(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "panjabi_tok = PanjabiTokenizer('pa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(tok_func=PanjabiTokenizer, lang='pa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxunk',\n",
       " 'xxpad',\n",
       " 'xxbos',\n",
       " 'xxeos',\n",
       " 'xxfld',\n",
       " 'xxmaj',\n",
       " 'xxup',\n",
       " 'xxrep',\n",
       " 'xxwrep']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = TextLMDataBunch.from_folder(path=path/'transformer', tokenizer=tokenizer, vocab=panjabi_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>▁ਇਹ ▁ ਮਾਦਾਗਾਸਕਰ ▁ਦੇ ▁ਪੱਛਮ ▁ਵੱਲ ▁ਪੈਂਦਾ ▁ਹੈ ▁ਅਤੇ ▁ਜੋ ▁ਸਭ ▁ਤੋਂ ▁ਨੇੜਲ ੇ ▁ਟਾਪੂ , ▁ਮਾਰੀਸ਼ਸ ▁ਤੋਂ ▁200 ▁ਕਿ . ਮੀ . ▁ਦੱਖਣ - ਪੱਛਮ ▁ਵੱਲ ▁ਹੈ । ▁ਪ੍ਰਸ਼ਾਸਕੀ ▁ਤੌਰ ▁ਉੱਤੇ ▁ਇਹ ▁ਫ਼ਰਾਂਸ ▁ਦਾ ▁ਇੱਕ ▁ਵਿਦੇਸ਼ੀ ▁ਵਿਭਾਗ ▁ਹੈ । ▁ਹੋਰ ▁ਵਿਦੇਸ਼ੀ ▁ਵਿਭਾਗਾਂ ▁ਵਾਂਗ ▁ਇਹ ▁ਫ਼ਰਾਂਸ ▁ਦੇ ▁27 ▁ਖੇਤਰਾਂ ▁ਵਿੱਚੋਂ ▁ਇੱਕ ▁ਹੈ ▁ਅਤੇ ▁ਗਣਰਾਜ ▁ਦਾ ▁ਅਨਿੱਖੜਵ ਾਂ ▁ਹਿੱਸਾ ▁ਹੈ ▁ਜਿਸਦਾ ▁ਦਰਜਾ ▁ਉਹੀ ▁ਹੈ ▁ਜੋ ▁ਮਹਾਂਦੀਪ ੀ ▁ਯੂਰਪ ▁ਵਿੱਚ ▁ਸਥਿੱਤ ▁ਖੇਤਰਾਂ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>▁ਤਲਵੰਡੀ ▁ਸਾਬੋ , ਦੇ ▁ਨੇੜੇ ▁ਪਿੰਡ ▁ਵਾਂਗਰ ▁ਆ ▁ਵੱਸੇ ▁ਸਨ । ▁x x bo s ▁ਸਾਰਾ ▁ਜੋਜ਼ਫ ਼ ▁ਜਾਂ ▁ਸਾਰਾ ▁ਜੋਸਫ਼ ▁ਇੱਕ ▁ਮਲਿਆਲਮ ▁ਨਾਵਲਕਾਰ ▁ਅਤੇ ▁ਨਿੱਕੀ ▁ਕਹਾਣੀ ▁ਲੇਖਕ ▁ਹੈ । ▁ਉਸ ▁ਨੇ ▁ਆਪਣੇ ▁ਨਾਵਲ ▁ਪਰਮੇਸ਼ੁਰ ▁ਪਿਤਾ ▁ਦੀਆਂ ▁ਧੀਆਂ ▁ਲਈ ▁ਸਾਹਿਤ ▁ਅਕਾਦਮੀ ▁ਅਵਾਰਡ ▁ਜਿੱਤਿਆ । ▁ਉਸੇ ▁ਹੀ ▁ਨਾਵਲ ▁ਲਈ ▁ਉਸਨੇ ▁ਵਾਇ ਲਾ ਰ ▁ਅਵਾਰਡ ▁ਵੀ ▁ਪ੍ਰਾਪਤ ▁ਕੀਤਾ । ▁ਸਾਰਾਹ ▁ਕੇਰਲਾ ▁ਵਿੱਚ ▁ਨਾਰੀਵਾਦੀ ▁ਲਹਿਰ ▁ਦੀ ▁ਮੋਹਰੀ ▁ਹੈ ▁ਅਤੇ ▁ਸੋਚ ਵਾਨ ▁ਮਹਿਲਾਵਾਂ ▁ਦੇ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>▁ਸੰਸਥਾ ਈ ▁ਦੇ ▁ਅਰਥਾਂ ▁ਵਿੱਚ ▁ਪ੍ਰਭਾਸ਼ਿਤ ▁ਕੀਤਾ ▁ਜਾਂਦਾ ▁ਹੈ ▁ਪ੍ਰ ੰਤੂ ▁ਮਾਰਕਸ ੀ ▁ਚਿੰਤਨ ▁ਦੇ ▁ਦੋਰਾਨ ▁ਇਹ ▁ਸੰਕਲਪ ▁ਵਿਚਾਰਧਾਰਕ ▁ਅਰਥਾਂ ▁ਵਿੱਚ ▁ਵਰਤਿਆ ▁ਗਿਆ ▁ਹੈ । ▁ਗ੍ਰਾਮ ਸ਼ੀ ▁ਦੇ ▁ਅਨੁਸਾਰ ▁ਇਹ ▁ਸੰਕਲਪ ▁ਕੇਵਲ ▁ਸਮਝਣ ▁ਵਿੱਚ ▁ਹੀ ▁ਨਹੀਂ ▁ਸਗੋਂ ▁ਸੱਤਾ ▁ਪਰਿਵਰਤਨ ▁ਵਿੱਚ ▁ਵਧੇਰੇ ▁ਮਦਦਗਾਰ ▁ਹੋ ▁ਸਕਦਾ ▁ਹੈ ▁। ▁ਸੱਤਾ ▁ਸ਼ੈਲੀ ▁ਜਮਾਤ ▁ਵਿੱਚ ▁ਹੈ ਜ ਮਨੀ ▁ਦੇ ▁ਨਾਲ ▁ਸੱਤਾ ▁ਨੂੰ ▁ਚਿਰਸਥਾਈ ▁ਬਣਾਉਦੀ ▁ਹੈ । ਹੈ ਜ ਮਨੀ ▁ਪੂੰਜੀਵਾਦ ▁ਦਾ ▁ਸੁਰਖਿਆ ▁ਕਵਚ ▁ਹੈ ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>▁130 ▁ਯਾਤਰੀ ▁ਸੈਰਗਾਹ ਾਂ ▁ਦਾ ▁ਛੋਟਾ ▁ਨਮੂਨਾ ▁ਤਿਆਰ ▁ਕੀਤਾ ▁ਗਿਆ ▁ਹੈ । ▁ਇਹ ▁ਥੀਮ ▁48 ▁ਹੈਕਟੇਅਰ ▁ਵਿੱਚ ▁ਬਣਿਆ ▁ਹੋਇਆ ▁ਹੈ । ▁ਇੱਥੇ ▁108 ▁ਮੀਟਰ ▁ਉੱਚਾ ▁ਆਈ ਫ਼ਲ ▁ਟਾਵਰ , ▁ਪਿਰਾਮਿਡ ▁ਤੇ ▁ਤਾਜ ▁ਮਹਿਲ ▁ਵੀ ▁ਬਣਾਇਆ ▁ਗਿਆ ▁ਹੈ । ▁ਸੰਸਾਰ ▁ਦੀ ▁ਖਿੜਕੀ ▁ਸ਼ੈਨ ਜ਼ ੈਨ ▁ਮੈਟਰੋ ▁ਦੀ ▁ਲਾਈਨ ▁1 ▁ਤੇ ▁ਲਾਈਨ ▁2 ▁ਕੋਲ ▁ਸਥਿੱਤ ▁ਹੈ । ▁ਹੈ ਪੀ - ਲਾਈਨ ▁ਨਾਂਅ ▁ਦੀ ▁ਛੋਟੀ ▁ਰੇਲ ▁ਸੰਸਾਰ ▁ਦੀ ▁ਖਿੜਕੀ ▁ਦੇ ▁ਨੇੜੇ ▁ਰੁਕ ਦੀ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>▁ਤੱਕ ▁ਵੈਦਿਕ ▁ਕਰਮ - ਕਾਂ ਡ ਾਂ ▁ਦਾ ▁ਅਸਰ ▁ਘੱਟ ▁ਹੋ ▁ਗਿਆ । ▁ਇਸਦੇ ▁ਫਲਸਰੂਪ ▁ਕਈ ▁ਧਾਰਮਿਕ ▁ਪੰਥਕ ਾਂ ▁ਅਤੇ ▁ਸੰਪਰਦਾ ਵਾਂ ▁ਦੀ ▁ਸਥਾਪਨਾ ▁ਹੋ ▁ਗਈ । ▁ੳੁਸ ▁ਸਮੇਂ ▁ਦੀਆਂ ▁ਲਗਭਗ ▁62 ▁ਸੰਪਰਦਾ ਵਾਂ ▁ਦੇ ▁ਬਾਰੇ ▁ਜਾਣਕਾਰੀ ▁ਮਿਲਦੀ ▁ਹੈ । ▁ਪਰ ▁ਇਨ੍ਹਾਂ ▁ਵਿਚੋਂ ▁ਦੋ ▁ਹੀ ▁ਲੰਬੇ ▁ਸਮੇਂ ▁ਤੱਕ ▁ਪ੍ਰਭਾਵਿਤ ▁ਕਰ ▁ਸਕੀਆਂ ▁- ▁ਬੁੱ । ਧ ▁ਅਤੇ ▁ਜੈਨ ▁ਜੈਨ ▁ਧਰਮ ▁ਦੇ ▁ਦੋ ▁ਤੀਰਥ ਕਰ ▁ਰਿਸ਼ਭ ਨਾਥ ▁ਅਤੇ ▁ਅਰ ਿਸ਼ਟ ਨੇ ਮੀ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "??language_model_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_lm.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, TransformerXL, pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): TransformerXL(\n",
       "    (encoder): Embedding(30000, 410)\n",
       "    (pos_enc): PositionalEncoding()\n",
       "    (drop_emb): Dropout(p=0.1)\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=410, out_features=30000, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.499693</td>\n",
       "      <td>5.407797</td>\n",
       "      <td>0.228888</td>\n",
       "      <td>18:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.762620</td>\n",
       "      <td>4.715902</td>\n",
       "      <td>0.303226</td>\n",
       "      <td>18:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.169302</td>\n",
       "      <td>4.087711</td>\n",
       "      <td>0.375758</td>\n",
       "      <td>18:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.883153</td>\n",
       "      <td>3.865367</td>\n",
       "      <td>0.399931</td>\n",
       "      <td>18:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.746451</td>\n",
       "      <td>3.757675</td>\n",
       "      <td>0.404383</td>\n",
       "      <td>18:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.530465</td>\n",
       "      <td>3.615756</td>\n",
       "      <td>0.418487</td>\n",
       "      <td>18:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.605102</td>\n",
       "      <td>3.536153</td>\n",
       "      <td>0.425905</td>\n",
       "      <td>18:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.649818</td>\n",
       "      <td>3.453868</td>\n",
       "      <td>0.434440</td>\n",
       "      <td>18:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.313667</td>\n",
       "      <td>3.344221</td>\n",
       "      <td>0.449939</td>\n",
       "      <td>18:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.279520</td>\n",
       "      <td>3.238690</td>\n",
       "      <td>0.463551</td>\n",
       "      <td>18:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.198136</td>\n",
       "      <td>3.137249</td>\n",
       "      <td>0.479374</td>\n",
       "      <td>18:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.931160</td>\n",
       "      <td>3.031256</td>\n",
       "      <td>0.494631</td>\n",
       "      <td>18:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.080706</td>\n",
       "      <td>2.949065</td>\n",
       "      <td>0.507558</td>\n",
       "      <td>18:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.790811</td>\n",
       "      <td>2.862436</td>\n",
       "      <td>0.521524</td>\n",
       "      <td>18:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.901925</td>\n",
       "      <td>2.789933</td>\n",
       "      <td>0.533742</td>\n",
       "      <td>18:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.715464</td>\n",
       "      <td>2.729868</td>\n",
       "      <td>0.543794</td>\n",
       "      <td>18:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.600902</td>\n",
       "      <td>2.681925</td>\n",
       "      <td>0.552044</td>\n",
       "      <td>18:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.542700</td>\n",
       "      <td>2.655642</td>\n",
       "      <td>0.556689</td>\n",
       "      <td>18:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.492688</td>\n",
       "      <td>2.643414</td>\n",
       "      <td>0.559015</td>\n",
       "      <td>18:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.222088</td>\n",
       "      <td>2.641254</td>\n",
       "      <td>0.559317</td>\n",
       "      <td>18:31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with accuracy value: 0.22888822853565216.\n",
      "Better model found at epoch 1 with accuracy value: 0.3032262623310089.\n",
      "Better model found at epoch 2 with accuracy value: 0.3757582902908325.\n",
      "Better model found at epoch 3 with accuracy value: 0.3999314606189728.\n",
      "Better model found at epoch 4 with accuracy value: 0.4043833017349243.\n",
      "Better model found at epoch 5 with accuracy value: 0.4184872806072235.\n",
      "Better model found at epoch 6 with accuracy value: 0.4259050488471985.\n",
      "Better model found at epoch 7 with accuracy value: 0.43444037437438965.\n",
      "Better model found at epoch 8 with accuracy value: 0.4499393701553345.\n",
      "Better model found at epoch 9 with accuracy value: 0.46355146169662476.\n",
      "Better model found at epoch 10 with accuracy value: 0.4793740212917328.\n",
      "Better model found at epoch 11 with accuracy value: 0.4946306347846985.\n",
      "Better model found at epoch 12 with accuracy value: 0.5075575709342957.\n",
      "Better model found at epoch 13 with accuracy value: 0.5215235352516174.\n",
      "Better model found at epoch 14 with accuracy value: 0.5337419509887695.\n",
      "Better model found at epoch 15 with accuracy value: 0.5437943339347839.\n",
      "Better model found at epoch 16 with accuracy value: 0.5520436763763428.\n",
      "Better model found at epoch 17 with accuracy value: 0.5566886067390442.\n",
      "Better model found at epoch 18 with accuracy value: 0.559014618396759.\n",
      "Better model found at epoch 19 with accuracy value: 0.5593172311782837.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(20, 1e-3, moms=(0.8,0.7), callbacks=[callbacks.SaveModelCallback(learn, every='improvement', monitor='accuracy', name='model')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ਜੋ ਉਹਨਾਂ ਦੇ ਰੱਬਾਂ ਨੂੰ ਪ੍ਰਸਤੁਤ ਕਰਦੇ ▁ਹਨ ▁ਜਿਹੜੇ ▁ਜਾਨਵਰ ▁ਜਾਂ ▁ਭੂ ਰੀਆ ▁ਦੀ ▁ਪਛਾਣ ▁ਕਰਨ ▁ਦੇ ▁ਯੋਗ ▁ਹਨ ▁1821 ▁ਵਿੱਚ ▁ਚਿ ਰਮ ▁ਅਜ ▁ਕੋਲੋਂ ੀਆਂ ▁ਦਾ'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict(\"ਜੋ ਉਹਨਾਂ ਦੇ ਰੱਬਾਂ ਨੂੰ ਪ੍ਰਸਤੁਤ ਕਰਦੇ\", n_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.030787187658118"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(2.641254)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults.device = torch.device('cpu')\n",
    "learn.model.eval()\n",
    "learn.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/gaurav/PycharmProjects/nlp-for-panjabi/language-model')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = get_model(learn.model)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30000, 410])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.state_dict()['encoder.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = encoder.state_dict()['encoder.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 400)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('embeddings.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>390</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.133232</td>\n",
       "      <td>1.795758</td>\n",
       "      <td>-0.899532</td>\n",
       "      <td>-0.563673</td>\n",
       "      <td>0.403549</td>\n",
       "      <td>0.804153</td>\n",
       "      <td>-0.491941</td>\n",
       "      <td>-0.442758</td>\n",
       "      <td>-0.541208</td>\n",
       "      <td>1.271042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163930</td>\n",
       "      <td>1.194224</td>\n",
       "      <td>-0.826213</td>\n",
       "      <td>0.932382</td>\n",
       "      <td>0.495702</td>\n",
       "      <td>-1.403555</td>\n",
       "      <td>-0.276732</td>\n",
       "      <td>-0.866433</td>\n",
       "      <td>0.196170</td>\n",
       "      <td>0.997835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.102023</td>\n",
       "      <td>-0.200170</td>\n",
       "      <td>-0.100507</td>\n",
       "      <td>-0.075280</td>\n",
       "      <td>-0.013930</td>\n",
       "      <td>0.011004</td>\n",
       "      <td>-0.321976</td>\n",
       "      <td>-0.028538</td>\n",
       "      <td>-0.090052</td>\n",
       "      <td>0.169326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097272</td>\n",
       "      <td>0.060103</td>\n",
       "      <td>-0.010100</td>\n",
       "      <td>0.206281</td>\n",
       "      <td>0.026072</td>\n",
       "      <td>0.051377</td>\n",
       "      <td>0.251827</td>\n",
       "      <td>0.038020</td>\n",
       "      <td>0.080573</td>\n",
       "      <td>1.347117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.088380</td>\n",
       "      <td>-0.182676</td>\n",
       "      <td>-0.086473</td>\n",
       "      <td>-0.097441</td>\n",
       "      <td>0.001567</td>\n",
       "      <td>-0.007080</td>\n",
       "      <td>-0.319105</td>\n",
       "      <td>-0.017408</td>\n",
       "      <td>-0.092607</td>\n",
       "      <td>0.166696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094963</td>\n",
       "      <td>0.050922</td>\n",
       "      <td>-0.012394</td>\n",
       "      <td>0.200892</td>\n",
       "      <td>0.022970</td>\n",
       "      <td>0.065053</td>\n",
       "      <td>0.284788</td>\n",
       "      <td>0.033773</td>\n",
       "      <td>0.083449</td>\n",
       "      <td>1.346565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.476688</td>\n",
       "      <td>-0.422615</td>\n",
       "      <td>-0.328648</td>\n",
       "      <td>-0.266224</td>\n",
       "      <td>-1.884391</td>\n",
       "      <td>-0.559162</td>\n",
       "      <td>-0.933407</td>\n",
       "      <td>-0.171928</td>\n",
       "      <td>0.631075</td>\n",
       "      <td>0.312376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120171</td>\n",
       "      <td>0.592503</td>\n",
       "      <td>-1.093363</td>\n",
       "      <td>-0.537873</td>\n",
       "      <td>0.290434</td>\n",
       "      <td>0.969293</td>\n",
       "      <td>-1.277264</td>\n",
       "      <td>-0.832494</td>\n",
       "      <td>-0.093807</td>\n",
       "      <td>0.799227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.657479</td>\n",
       "      <td>0.592319</td>\n",
       "      <td>0.293377</td>\n",
       "      <td>0.019757</td>\n",
       "      <td>1.040759</td>\n",
       "      <td>-0.034394</td>\n",
       "      <td>-1.003682</td>\n",
       "      <td>-0.049248</td>\n",
       "      <td>0.261375</td>\n",
       "      <td>-0.008441</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128985</td>\n",
       "      <td>-0.173574</td>\n",
       "      <td>-0.774097</td>\n",
       "      <td>0.225861</td>\n",
       "      <td>0.549480</td>\n",
       "      <td>0.433734</td>\n",
       "      <td>-0.825751</td>\n",
       "      <td>-1.694699</td>\n",
       "      <td>0.067637</td>\n",
       "      <td>0.996539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.133232  1.795758 -0.899532 -0.563673  0.403549  0.804153 -0.491941   \n",
       "1 -0.102023 -0.200170 -0.100507 -0.075280 -0.013930  0.011004 -0.321976   \n",
       "2 -0.088380 -0.182676 -0.086473 -0.097441  0.001567 -0.007080 -0.319105   \n",
       "3 -0.476688 -0.422615 -0.328648 -0.266224 -1.884391 -0.559162 -0.933407   \n",
       "4  0.657479  0.592319  0.293377  0.019757  1.040759 -0.034394 -1.003682   \n",
       "\n",
       "        7         8         9    ...       390       391       392       393  \\\n",
       "0 -0.442758 -0.541208  1.271042  ...  0.163930  1.194224 -0.826213  0.932382   \n",
       "1 -0.028538 -0.090052  0.169326  ...  0.097272  0.060103 -0.010100  0.206281   \n",
       "2 -0.017408 -0.092607  0.166696  ...  0.094963  0.050922 -0.012394  0.200892   \n",
       "3 -0.171928  0.631075  0.312376  ...  0.120171  0.592503 -1.093363 -0.537873   \n",
       "4 -0.049248  0.261375 -0.008441  ... -0.128985 -0.173574 -0.774097  0.225861   \n",
       "\n",
       "        394       395       396       397       398       399  \n",
       "0  0.495702 -1.403555 -0.276732 -0.866433  0.196170  0.997835  \n",
       "1  0.026072  0.051377  0.251827  0.038020  0.080573  1.347117  \n",
       "2  0.022970  0.065053  0.284788  0.033773  0.083449  1.346565  \n",
       "3  0.290434  0.969293 -1.277264 -0.832494 -0.093807  0.799227  \n",
       "4  0.549480  0.433734 -0.825751 -1.694699  0.067637  0.996539  \n",
       "\n",
       "[5 rows x 400 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 400)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0  <unk>\n",
       "1    <s>\n",
       "2   </s>\n",
       "3      ।\n",
       "4      ,"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('embeddings_metadata.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0202e-01, -2.0017e-01, -1.0051e-01, -7.5280e-02, -1.3930e-02,\n",
       "         1.1004e-02, -3.2198e-01, -2.8538e-02, -9.0052e-02,  1.6933e-01,\n",
       "         1.2757e-01,  1.6098e-02,  5.0098e-02, -8.6869e-02, -8.0517e-02,\n",
       "        -2.1752e-01, -1.1775e-01, -5.5983e-02,  3.2229e-02, -2.7267e-02,\n",
       "        -1.0095e-01, -4.7528e-02, -2.1485e-01, -2.8627e-02, -2.4670e-01,\n",
       "        -1.2306e-01,  1.1774e-01,  2.6725e-01, -1.5533e-01, -6.9340e-02,\n",
       "         8.0709e-02,  1.0343e-01, -2.0692e-04,  5.3577e-01,  1.9114e-01,\n",
       "        -6.2939e-02, -3.3008e-01, -1.2618e-01, -2.9081e-01,  1.1732e-01,\n",
       "        -2.5975e-02,  1.3646e-01,  3.5592e-01,  3.6853e-01,  3.2732e-01,\n",
       "         7.6069e-02,  2.9203e-01, -1.4280e-01,  1.2574e-02,  3.5729e-02,\n",
       "        -2.9483e-01,  9.3886e-03,  4.2057e-01,  9.1104e-02,  3.3001e-01,\n",
       "        -2.1850e-01, -1.7247e-01, -2.7076e-01, -6.1937e-02,  4.6683e-02,\n",
       "         3.4150e-01,  5.0922e-02, -8.9157e-02, -5.9138e-02, -2.5139e-01,\n",
       "         3.0671e-02,  1.3081e-01,  1.1541e-01, -5.6019e-02, -3.2664e-01,\n",
       "         2.0946e-01,  6.4147e-02, -2.5664e-01, -3.6984e-01, -3.5922e-01,\n",
       "         1.8282e-01, -5.7972e-03, -5.9358e-02, -2.1030e-01,  2.2410e-01,\n",
       "         1.6359e-02, -8.5969e-02,  1.3841e-01,  1.1337e-01, -1.8218e-02,\n",
       "         1.5936e-01,  2.6701e-01, -4.8380e-02, -5.2922e-02,  1.0642e-01,\n",
       "         1.3187e-01,  3.8934e-02, -2.9986e-01, -1.4837e-02, -3.1197e-01,\n",
       "         1.0771e-01, -1.9898e-01, -2.6639e-01,  7.6598e-02, -1.2081e-01,\n",
       "        -1.8872e-01, -5.7801e-01, -1.8027e-02, -2.5573e-01,  2.8595e-01,\n",
       "        -2.9008e-03,  5.5153e-02, -2.1879e-02,  8.9946e-02,  3.2526e-03,\n",
       "         2.0189e-01,  1.7897e-02, -1.6736e-01, -6.9565e-02, -1.4959e-03,\n",
       "         2.0300e-01, -4.4761e-02, -1.0238e-01, -2.5850e-02, -1.8756e-01,\n",
       "         3.7821e-02,  3.9299e-01,  4.5720e-02,  7.1159e-02, -5.8795e-02,\n",
       "        -3.8308e-02,  3.2736e-02, -3.5817e-01,  6.7470e-01,  9.8633e-03,\n",
       "         1.1797e-01, -1.4884e-01, -1.3176e-02, -3.2228e-02, -1.1344e-01,\n",
       "        -5.5961e-02, -3.4339e-01, -6.2342e-02,  2.7313e-01,  9.7456e-02,\n",
       "         1.1130e-01,  4.0733e-02,  4.3569e-03,  9.9798e-02, -1.7370e-01,\n",
       "        -9.9084e-02,  2.2026e-01, -1.1424e-02, -7.5381e-02, -7.8695e-02,\n",
       "        -4.4737e-02,  1.1547e-01, -6.9726e-02, -2.1852e-01,  6.1798e-01,\n",
       "        -5.4356e-02,  1.4022e-01,  5.0864e-02, -2.3300e-01, -9.0529e-03,\n",
       "         2.5653e-02, -6.3334e-02, -8.8149e-02,  6.7296e-02, -2.0190e-01,\n",
       "        -3.1794e-02,  1.4618e-02, -2.1261e-01, -2.2154e-01, -4.5354e-02,\n",
       "        -2.9290e-01, -3.8098e-02, -2.0497e-01,  1.9144e-01, -7.0808e-02,\n",
       "         7.4890e-02, -9.9078e-02, -1.5439e-01,  1.3136e-01,  1.4002e-01,\n",
       "        -1.4835e-01, -1.9062e-01,  1.8280e-01,  2.7940e-02,  5.7339e-02,\n",
       "         5.2006e-02,  5.3729e-02, -2.2099e-01, -1.3384e-01, -3.6734e-02,\n",
       "         2.9932e-02, -7.6444e-02,  4.2917e-02,  4.3837e-02, -8.3998e-03,\n",
       "        -3.1518e-02, -6.9764e-03,  1.3664e-01,  1.4279e-02,  4.3456e-03,\n",
       "        -3.5388e-02,  5.6459e-02, -4.1112e-03,  6.2927e-02, -4.3855e-01,\n",
       "         1.1950e-01, -1.0021e-01, -2.1730e-01,  1.2561e-02, -3.3522e-01,\n",
       "        -2.4554e-01,  1.1167e-01, -1.3407e-01, -1.7158e-01,  1.5716e-02,\n",
       "        -1.1008e-01, -5.1010e-01, -1.4399e-01, -3.2995e-02, -1.5877e-01,\n",
       "         1.6405e-01,  2.2335e-01,  1.6085e-01, -4.8433e-03, -1.4873e-01,\n",
       "         2.4449e-01,  1.5804e-01, -4.2449e-02, -1.8032e-01, -5.8260e-02,\n",
       "        -8.3427e-02, -2.1071e-01, -8.4662e-02, -3.5732e-01,  2.2357e-02,\n",
       "        -1.8980e-01,  4.4649e-01, -2.2912e-02, -4.2313e-02, -4.4187e-03,\n",
       "         2.3235e-01,  1.2261e-01, -1.0658e-01, -2.4763e-02,  8.0133e-02,\n",
       "        -1.1902e-01, -1.5742e-01,  1.9069e-02, -1.9580e-01,  1.8546e-01,\n",
       "        -1.6403e-03, -3.1231e-01,  3.6150e-02, -1.0447e-01, -2.9022e-02,\n",
       "         9.0345e-02,  6.5377e-01, -4.0721e-02, -1.1253e-01, -8.9744e-02,\n",
       "         1.9333e-02, -1.1239e-01,  3.6182e-01,  4.1823e-02,  3.4475e-01,\n",
       "         2.1092e-02,  8.0602e-02,  1.5772e-01,  6.9633e-02, -1.2902e-01,\n",
       "         3.3532e-01, -8.6293e-02, -1.1693e-01, -2.1198e-01, -5.5791e-02,\n",
       "        -3.2576e-02, -1.8825e-02, -1.6057e-02, -3.1696e-02, -3.8407e-01,\n",
       "         2.9580e-02, -5.3885e-02,  5.6598e-02, -1.6747e-01,  1.2041e-01,\n",
       "        -7.2858e-03,  3.6547e-02, -4.6202e-02, -3.2149e-01,  2.0245e-01,\n",
       "        -1.3208e-01, -5.3164e-02,  3.6536e-01,  2.2462e-01,  1.4292e-01,\n",
       "         5.6006e-02, -1.6561e-01, -4.4857e-02, -6.6184e-03, -2.2348e-02,\n",
       "        -1.3957e-01,  9.0584e-02,  1.6983e-01, -1.2745e-02, -2.7735e-01,\n",
       "         3.9728e-02, -1.4569e-01,  1.5244e-01, -2.8070e-01,  6.2365e-02,\n",
       "        -3.3694e-01, -1.2685e-01,  1.5060e-01, -9.4215e-01, -5.0718e-02,\n",
       "        -6.9665e-02, -4.5893e-02, -6.0775e-02, -5.4125e-02, -2.7715e-02,\n",
       "        -1.2354e-03, -2.2854e-01,  2.1378e-01, -2.3155e-01, -2.6047e-02,\n",
       "        -1.6361e-02,  1.5132e-01, -1.7087e-01, -2.2143e-01, -1.6505e-01,\n",
       "        -3.4881e-01,  9.8525e-02,  7.8785e-02, -4.8821e-02, -6.1839e-02,\n",
       "         4.1054e-02,  1.4070e-01,  1.3517e-01, -1.3419e-01, -2.3162e-02,\n",
       "        -3.2103e-01, -1.4084e-01,  1.2786e-01, -1.1062e-01, -5.1836e-03,\n",
       "        -1.2813e-01, -1.8376e-01,  2.1468e-01, -4.3905e-01, -1.6209e-03,\n",
       "        -3.1252e-01, -3.3605e-02,  5.6471e-02, -6.7410e-02, -5.0575e-02,\n",
       "        -1.7691e-01,  2.6675e-02, -2.4106e-01, -2.4739e-01, -7.7774e-02,\n",
       "        -1.2037e-01,  3.2247e-02, -9.3692e-03, -4.2931e-02, -5.5898e-02,\n",
       "         2.6404e-01,  2.6907e-01, -5.4089e-01, -1.2327e-01,  2.9651e-01,\n",
       "         6.7046e-01,  1.8173e-01, -6.6511e-02,  4.1194e-01,  2.3419e-04,\n",
       "        -2.4186e-01, -1.8745e-01,  2.2039e-01, -1.8893e-01,  1.0531e-01,\n",
       "        -3.9080e-01, -3.4104e-02,  1.5821e-01,  1.4427e-01, -4.3747e-02,\n",
       "         3.7401e-01,  2.8393e-01,  3.8395e-02,  1.3899e-01,  1.2006e-01,\n",
       "         9.7272e-02,  6.0103e-02, -1.0100e-02,  2.0628e-01,  2.6072e-02,\n",
       "         5.1377e-02,  2.5183e-01,  3.8020e-02,  8.0573e-02,  1.3471e+00],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.state_dict()['encoder.weight'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerXL(\n",
       "  (encoder): Embedding(30000, 410)\n",
       "  (pos_enc): PositionalEncoding()\n",
       "  (drop_emb): Dropout(p=0.1)\n",
       "  (layers): ModuleList(\n",
       "    (0): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1)\n",
       "        (drop_res): Dropout(p=0.1)\n",
       "        (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1)\n",
       "        (drop_res): Dropout(p=0.1)\n",
       "        (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1)\n",
       "        (drop_res): Dropout(p=0.1)\n",
       "        (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1)\n",
       "        (drop_res): Dropout(p=0.1)\n",
       "        (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1)\n",
       "        (drop_res): Dropout(p=0.1)\n",
       "        (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1)\n",
       "        (drop_res): Dropout(p=0.1)\n",
       "        (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1)\n",
       "        (drop_res): Dropout(p=0.1)\n",
       "        (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1)\n",
       "        (drop_res): Dropout(p=0.1)\n",
       "        (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1)\n",
       "        (drop_res): Dropout(p=0.1)\n",
       "        (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1)\n",
       "        (drop_res): Dropout(p=0.1)\n",
       "        (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1)\n",
       "        (drop_res): Dropout(p=0.1)\n",
       "        (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1)\n",
       "        (drop_res): Dropout(p=0.1)\n",
       "        (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
